# Databricks notebook source

%sh pip install pg-de-cf-databricks-configuration pg-de-cf-databricks-migration

# COMMAND ----------

from pg_composite_pipelines_migration.migration import Migration
from pg_composite_pipelines_configuration.configuration import Configuration

config = Configuration.load_for_default_environment_notebook(dbutils)
schema = f"{config['catalog-name']}.{config['schema-name']}"
{{- if eq .das_rls "yes" }}
view_schema = f"{config['catalog-name']}.{config['rls-schema-name']}"
{{- end}}


def migrate():
    storage_location = f"abfss://{config['storage']['container-name']}@{config['storage']['account-name']}.dfs.core.windows.net"
    {{- if eq .das_rls "yes" }}
    # migrate tables
    Migration.migrate(
        "{{.project_name}}/{{.schema_name}}_highly_restricted/ddl",
        schema,
        storage_location,
        spark=spark,
        dbutils=dbutils,
    )
    # migrate rls views
    Migration.migrate(
        "{{.project_name}}/{{.schema_name}}/ddl",
        view_schema,
        storage_location,
        spark=spark,
        dbutils=dbutils,
        schema_info="view_schema_info",
        hrschema=schema,
    )
    {{- else }}
    Migration.migrate(
        "{{.project_name}}/{{.schema_name}}/ddl",
        schema,
        storage_location,
        spark=spark,
        dbutils=dbutils,
    )
    {{- end}}


def demigrate():
    Migration.demigrate_to_sql_files(f"{schema}.%", "{{.project_name}}")


migrate()
